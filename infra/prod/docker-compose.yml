name: beefy-databarn

services:
  traefik:
    image: traefik:v3.6
    command:
      - '--configfile=/traefik.yml'
      - '--certificatesresolvers.letsencrypt.acme.email=${TRAEFIK_EMAIL}'
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - ../traefik/traefik.yml:/traefik.yml
      - traefik_certs:/letsencrypt
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      TRAEFIK_DOCKER_NETWORK: beefy-databarn_frontend_network
      TRAEFIK_LOG_LEVEL: ${TRAEFIK_LOG_LEVEL:-INFO}
      TRAEFIK_EMAIL: ${TRAEFIK_EMAIL}
    networks:
      - frontend_network
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ['CMD', 'traefik', 'healthcheck', '--ping']
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:25.10
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ../clickhouse/config.xml:/etc/clickhouse-server/config.xml:ro
      - ../clickhouse/users.xml:/etc/clickhouse-server/users.xml:ro
      - ../clickhouse/users.d:/etc/clickhouse-server/users.d:ro
      - ../clickhouse/init-clickhouse.sh:/docker-entrypoint-initdb.d/init-clickhouse.sh:ro
    environment:
      CLICKHOUSE_SKIP_USER_SETUP: 1
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      CLICKHOUSE_DB: ${CLICKHOUSE_DB:-analytics}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_MAX_MEMORY_USAGE: ${CLICKHOUSE_MAX_MEMORY_USAGE:-32212254720}
      CLICKHOUSE_MAX_CONNECTIONS: ${CLICKHOUSE_MAX_CONNECTIONS:-100}
      CLICKHOUSE_MAX_CONCURRENT_QUERIES: ${CLICKHOUSE_MAX_CONCURRENT_QUERIES:-100}
      CLICKHOUSE_UNCOMPRESSED_CACHE_SIZE: ${CLICKHOUSE_UNCOMPRESSED_CACHE_SIZE:-10737418240}
      CLICKHOUSE_MARK_CACHE_SIZE: ${CLICKHOUSE_MARK_CACHE_SIZE:-10737418240}
      CLICKHOUSE_ALWAYS_RUN_INITDB_SCRIPTS: ${CLICKHOUSE_ALWAYS_RUN_INITDB_SCRIPTS:-1}
      DBT_CLICKHOUSE_PASSWORD: ${DBT_CLICKHOUSE_PASSWORD}
      DBT_CLICKHOUSE_ALLOWED_HOST: IP '172.19.1.0/24'
      DLT_CLICKHOUSE_PASSWORD: ${DLT_CLICKHOUSE_PASSWORD}
      DLT_CLICKHOUSE_ALLOWED_HOST: IP '172.19.1.0/24'
      SUPERSET_CLICKHOUSE_PASSWORD: ${SUPERSET_CLICKHOUSE_PASSWORD}
      SUPERSET_CLICKHOUSE_ALLOWED_HOST: IP '172.19.1.0/24'
      GRAFANA_CLICKHOUSE_PASSWORD: ${GRAFANA_CLICKHOUSE_PASSWORD}
      GRAFANA_CLICKHOUSE_ALLOWED_HOST: IP '172.19.1.0/24'
      API_CLICKHOUSE_PASSWORD: ${API_CLICKHOUSE_PASSWORD}
      API_CLICKHOUSE_ALLOWED_HOST: IP '172.19.1.0/24'
      ENVIO_CLICKHOUSE_PASSWORD: ${ENVIO_CLICKHOUSE_PASSWORD}
      ENVIO_CLICKHOUSE_ALLOWED_HOST: IP '172.19.0.0/24' # external
      CLICKHOUSE_WEB_MAX_EXECUTION_TIME: ${CLICKHOUSE_WEB_MAX_EXECUTION_TIME:-3600}
      CLICKHOUSE_WEB_MAX_MEMORY_USAGE: ${CLICKHOUSE_WEB_MAX_MEMORY_USAGE:-32212254720}
      CLICKHOUSE_WEB_MAX_RESULT_ROWS: ${CLICKHOUSE_WEB_MAX_RESULT_ROWS:-1000000000}
      CLICKHOUSE_WEB_MAX_ROWS_TO_READ: ${CLICKHOUSE_WEB_MAX_ROWS_TO_READ:-1000000000}
    networks:
      - backend_network
      - frontend_network
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=beefy-databarn_frontend_network"
      - "traefik.http.routers.clickhouse.rule=Host(`${CLICKHOUSE_EXTERNAL_DOMAIN_NAME}`)"
      - "traefik.http.routers.clickhouse.entrypoints=websecure"
      - "traefik.http.routers.clickhouse.tls.certresolver=letsencrypt"
      - "traefik.http.routers.clickhouse.middlewares=clickhouse-ratelimit"
      - "traefik.http.middlewares.clickhouse-ratelimit.ratelimit.average=300"
      - "traefik.http.middlewares.clickhouse-ratelimit.ratelimit.period=1s"
      - "traefik.http.middlewares.clickhouse-ratelimit.ratelimit.burst=100"
      - "traefik.http.middlewares.clickhouse-ratelimit.ratelimit.sourcecriterion.ipstrategy.depth=1"
      - "traefik.http.services.clickhouse.loadbalancer.server.port=8123"
      - "traefik.http.services.clickhouse.loadbalancer.server.scheme=http"
      - "traefik.http.services.clickhouse.loadbalancer.healthcheck.path=/ping"
      - "traefik.http.services.clickhouse.loadbalancer.healthcheck.interval=10s"
      - "traefik.http.services.clickhouse.loadbalancer.healthcheck.timeout=5s"
      - "traefik.http.routers.clickhouse.service=clickhouse"
    healthcheck:
      test: wget -O - --no-verbose --tries=1 http://127.0.0.1:8123/ping || exit 1
      interval: 1s
      timeout: 20s
      retries: 10
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  prometheus:
    image: prom/prometheus:v3.7.3
    volumes:
      - prometheus_data:/prometheus
      - ../prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ../prometheus/alerts.yml:/etc/prometheus/alerts.yml
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    networks:
      - backend_network
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/metrics"]
      interval: 1s
      timeout: 20s
      retries: 10
    depends_on:
      clickhouse:
        condition: service_healthy
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:12.2
    volumes:
      - grafana_data:/var/lib/grafana
      - ../grafana/provisioning:/etc/grafana/provisioning
      - ../grafana/dashboards:/etc/grafana/dashboards
      - ../grafana/custom.ini:/etc/grafana/grafana.ini
    environment:
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL}
      GF_SERVER_DOMAIN: ${GRAFANA_WEBAPP_DOMAIN_NAME}
      GF_LOG_LEVEL: ${GRAFANA_LOG_LEVEL:-info}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_SECRETS_MANAGER_ENCRYPTION_SECRET_KEY_V1: ${GRAFANA_SECRET_KEY}
      GF_SERVER_HTTP_ADDR: ${GRAFANA_HTTP_ADDR:-0.0.0.0}
      GF_SERVER_HTTP_PORT: ${GRAFANA_HTTP_PORT:-3000}
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_ADMIN_EMAIL: ${GRAFANA_ADMIN_EMAIL:-admin@localhost}
      GF_DATABASE_HOST: ${GRAFANA_POSTGRES_HOST:-postgres}
      GF_DATABASE_PORT: ${GRAFANA_POSTGRES_PORT:-5432}
      GF_DATABASE_USER: ${GRAFANA_POSTGRES_USER:-grafana}
      GF_DATABASE_PASSWORD: ${GRAFANA_POSTGRES_PASSWORD}
      GF_DATABASE_NAME: ${GRAFANA_POSTGRES_DB:-grafana}
      GF_REMOTE_CACHE_CONNSTR: ${GRAFANA_REDIS_CONNSTR:-addr=redis:6379,pool_size=100,db=2,ssl=false}
      GF_REMOTE_CACHE_PREFIX: ${GRAFANA_REDIS_PREFIX:-grafana:}
      GF_AUTH_GITHUB_CLIENT_ID: ${GRAFANA_GITHUB_CLIENT_ID}
      GF_AUTH_GITHUB_CLIENT_SECRET: ${GRAFANA_GITHUB_CLIENT_SECRET}
      GF_AUTH_GITHUB_ALLOWED_ORGANIZATIONS: ${GRAFANA_GITHUB_ALLOWED_ORG}
      # datasource config
      GRAFANA_CLICKHOUSE_HOST: ${GRAFANA_CLICKHOUSE_HOST:-clickhouse}
      GRAFANA_CLICKHOUSE_PORT: 8123
      GRAFANA_CLICKHOUSE_USER: ${GRAFANA_CLICKHOUSE_USER:-grafana}
      GRAFANA_CLICKHOUSE_PASSWORD: ${GRAFANA_CLICKHOUSE_PASSWORD}
      GRAFANA_CLICKHOUSE_DB: ${GRAFANA_CLICKHOUSE_DB:-analytics}
    networks:
      - backend_network
      - frontend_network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`${GRAFANA_WEBAPP_DOMAIN_NAME}`)"
      - "traefik.http.routers.grafana.entrypoints=websecure"
      - "traefik.http.routers.grafana.tls.certresolver=letsencrypt"
      - "traefik.http.services.grafana.loadbalancer.server.port=${GRAFANA_HTTP_PORT:-3000}"
      - "traefik.http.services.grafana.loadbalancer.server.scheme=http"
      - "traefik.http.services.grafana.loadbalancer.healthcheck.path=/api/health"
      - "traefik.http.services.grafana.loadbalancer.healthcheck.interval=10s"
      - "traefik.http.services.grafana.loadbalancer.healthcheck.timeout=5s"
      - "traefik.http.routers.grafana.service=grafana"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1",
        ]
      interval: 1s
      timeout: 20s
      retries: 10
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"
    restart: unless-stopped

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    command: server /data --console-address ":9001"
    entrypoint: ["/entrypoint.sh"]
    volumes:
      - minio_data:/data
      - ../minio/entrypoint.sh:/entrypoint.sh
    environment:
      MINIO_BROWSER: off
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-admin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
      MINIO_DLT_STAGING_BUCKET: ${MINIO_DLT_STAGING_BUCKET:-dlt_staging}
      MINIO_PROMETHEUS_AUTH_TYPE: public
    networks:
      - backend_network
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 1s
      timeout: 20s
      retries: 10
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"
    restart: unless-stopped

  postgres:
    image: pgautoupgrade/pgautoupgrade:17-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../postgres/init-postgres.sh:/docker-entrypoint-initdb.d/init-postgres.sh
    environment:
      POSTGRES_DB: ${POSTGRES_ADMIN_DB:-postgres}
      POSTGRES_USER: ${POSTGRES_ADMIN_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_ADMIN_PASSWORD:-postgres}
      SUPERSET_POSTGRES_DB: ${SUPERSET_POSTGRES_DB:-superset}
      SUPERSET_POSTGRES_USER: ${SUPERSET_POSTGRES_USER:-superset}
      SUPERSET_POSTGRES_PASSWORD: ${SUPERSET_POSTGRES_PASSWORD}
      GRAFANA_POSTGRES_DB: ${GRAFANA_POSTGRES_DB:-grafana}
      GRAFANA_POSTGRES_USER: ${GRAFANA_POSTGRES_USER:-grafana}
      GRAFANA_POSTGRES_PASSWORD: ${GRAFANA_POSTGRES_PASSWORD}
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_ADMIN_USER:-postgres}"]
      interval: 5s
      timeout: 5s
      retries: 5
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  redis:
    image: redis:8-alpine
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  redis_exporter:
    image: oliver006/redis_exporter:latest
    environment:
      REDIS_ADDR: redis:6379
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    depends_on:
      redis:
        condition: service_healthy
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:v0.18.1
    environment:
      DATA_SOURCE_NAME: postgresql://${POSTGRES_ADMIN_USER:-postgres}:${POSTGRES_ADMIN_PASSWORD:-postgres}@postgres:5432/${POSTGRES_ADMIN_DB:-postgres}?sslmode=disable
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9187/metrics"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      postgres:
        condition: service_healthy
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  superset:
    build:
      context: ../superset
      dockerfile: Dockerfile
    image: beefy-databarn-superset:latest
    volumes:
      - superset_data:/app/superset_home
      - ../superset/provision:/usr/local/bin/provision
      - ../superset/docker-entrypoint-wrapper.sh:/app/docker-entrypoint-wrapper.sh
      - ../superset/superset_config.py:/app/superset_config.py
      - ../superset/github_security_manager.py:/app/pythonpath/github_security_manager.py
    command: ["/app/docker-entrypoint-wrapper.sh"]
    environment:
      SUPERSET_WEBAPP_DOMAIN_NAME: ${SUPERSET_WEBAPP_DOMAIN_NAME}
      SUPERSET_ENV: ${SUPERSET_ENV:-production}
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_CONFIG_PATH: /app/superset_config.py
      POSTGRES_USER: ${SUPERSET_POSTGRES_USER:-superset}
      POSTGRES_PASSWORD: ${SUPERSET_POSTGRES_PASSWORD}
      POSTGRES_DB: ${SUPERSET_POSTGRES_DB:-superset}
      SUPERSET_POSTGRES_HOST: postgres
      SUPERSET_POSTGRES_PORT: 5432
      SUPERSET_POSTGRES_USER: ${SUPERSET_POSTGRES_USER:-superset}
      SUPERSET_POSTGRES_PASSWORD: ${SUPERSET_POSTGRES_PASSWORD}
      SUPERSET_POSTGRES_DB: ${SUPERSET_POSTGRES_DB:-superset}
      SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@example.com}
      SUPERSET_CLICKHOUSE_HOST: ${SUPERSET_CLICKHOUSE_HOST}
      SUPERSET_CLICKHOUSE_PORT: 8123
      SUPERSET_CLICKHOUSE_USER: ${SUPERSET_CLICKHOUSE_USER:-superset}
      SUPERSET_CLICKHOUSE_PASSWORD: ${SUPERSET_CLICKHOUSE_PASSWORD}
      SUPERSET_CLICKHOUSE_DB: ${SUPERSET_CLICKHOUSE_DB:-analytics}
      SUPERSET_REDIS_HOST: redis
      SUPERSET_REDIS_PORT: 6379
      SUPERSET_REDIS_CELERY_DB: ${SUPERSET_REDIS_CELERY_DB:-0}
      SUPERSET_REDIS_RESULTS_DB: ${SUPERSET_REDIS_RESULTS_DB:-1}
      # GitHub OAuth Configuration
      SUPERSET_GITHUB_CLIENT_ID: ${SUPERSET_GITHUB_CLIENT_ID:-}
      SUPERSET_GITHUB_CLIENT_SECRET: ${SUPERSET_GITHUB_CLIENT_SECRET:-}
      SUPERSET_GITHUB_API_BASE_URL: ${SUPERSET_GITHUB_API_BASE_URL:-}
      SUPERSET_GITHUB_ACCESS_TOKEN_URL: ${SUPERSET_GITHUB_ACCESS_TOKEN_URL:-}
      SUPERSET_GITHUB_AUTHORIZE_URL: ${SUPERSET_GITHUB_AUTHORIZE_URL:-}
      SUPERSET_GITHUB_ALLOWED_ORG: ${SUPERSET_GITHUB_ALLOWED_ORG:-}
      SUPERSET_AUTH_USER_REGISTRATION_ROLE: ${SUPERSET_AUTH_USER_REGISTRATION_ROLE:-Gamma}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/health || exit 1"]
      interval: 1s
      timeout: 10s
      retries: 20
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.superset.rule=Host(`${SUPERSET_WEBAPP_DOMAIN_NAME}`)"
      - "traefik.http.routers.superset.entrypoints=websecure"
      - "traefik.http.routers.superset.tls.certresolver=letsencrypt"
      - "traefik.http.services.superset.loadbalancer.server.port=8088"
      - "traefik.http.services.superset.loadbalancer.server.scheme=http"
      - "traefik.http.services.superset.loadbalancer.healthcheck.path=/health"
      - "traefik.http.services.superset.loadbalancer.healthcheck.interval=10s"
      - "traefik.http.services.superset.loadbalancer.healthcheck.timeout=5s"
      - "traefik.http.routers.superset.service=superset"
    networks:
      - backend_network
      - frontend_network
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"
    restart: unless-stopped

  dbt:
    build:
      context: ../..
      dockerfile: infra/dbt/Dockerfile
    image: beefy-databarn-dbt:latest
    volumes:
      - dbt_data:/app/dbt/target
      - ../../dbt:/app/dbt
    environment:
      DBT_TARGET: prod
      DBT_RUN_LOCKFILE: ${DBT_RUN_LOCKFILE:-/app/dbt/.dbt_run.lock}
      DBT_RUN_LOCK_TIMEOUT: ${DBT_RUN_LOCK_TIMEOUT:-30}
      DBT_CLICKHOUSE_HOST: ${DBT_CLICKHOUSE_HOST:-clickhouse}
      DBT_CLICKHOUSE_PORT: 8123
      DBT_CLICKHOUSE_USER: ${DBT_CLICKHOUSE_USER:-dbt}
      DBT_CLICKHOUSE_PASSWORD: ${DBT_CLICKHOUSE_PASSWORD}
      DBT_CLICKHOUSE_DB: ${DBT_CLICKHOUSE_DB:-analytics}
      DBT_ENVIO_BALANCES_DB: ${DBT_ENVIO_BALANCES_DB:-envio}
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    depends_on:
      clickhouse:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'scheduler.py' > /dev/null && exit 0 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  dlt:
    build:
      context: ../..
      dockerfile: infra/dlt/Dockerfile
    image: beefy-databarn-dlt:latest
    volumes:
      - ../../dlt:/app/dlt
      # DLT writes pipeline data (load/loaded/completed_jobs) to /var/dlt â€“ mount volume so it does not fill the container overlay
      - dlt_data:/var/dlt
    environment:
      DLT_ENV: ${DLT_ENV:-production}
      STORAGE_DIR: ${STORAGE_DIR}
      # ClickHouse configuration
      DLT_CLICKHOUSE_HOST: ${DLT_CLICKHOUSE_HOST:-clickhouse}
      DLT_CLICKHOUSE_PORT: ${DLT_CLICKHOUSE_PORT:-9000}
      DLT_CLICKHOUSE_HTTP_PORT: ${DLT_CLICKHOUSE_HTTP_PORT:-8123}
      DLT_CLICKHOUSE_USER: ${DLT_CLICKHOUSE_USER:-dlt}
      DLT_CLICKHOUSE_PASSWORD: ${DLT_CLICKHOUSE_PASSWORD}
      DLT_CLICKHOUSE_DB: ${DLT_CLICKHOUSE_DB:-dlt}
      # beefy db source configuration
      BEEFY_DB_HOST: ${BEEFY_DB_HOST:-postgres}
      BEEFY_DB_PORT: ${BEEFY_DB_PORT:-5432}
      BEEFY_DB_NAME: ${BEEFY_DB_NAME:-beefy-db}
      BEEFY_DB_USER: ${BEEFY_DB_USER:-postgres}
      BEEFY_DB_PASSWORD: ${BEEFY_DB_PASSWORD}
      # minio filesystem destination configuration
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-admin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
      MINIO_DLT_STAGING_BUCKET: ${MINIO_DLT_STAGING_BUCKET:-dlt-staging}
    networks:
      - backend_network
    restart: on-failure
    labels:
      - "traefik.enable=false"
    depends_on:
      clickhouse:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'scheduler.py' > /dev/null && exit 0 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"

  api:
    build:
      context: ../..
      dockerfile: infra/api/Dockerfile
    image: beefy-databarn-api:latest
    environment:
      API_CLICKHOUSE_HOST: ${API_CLICKHOUSE_HOST:-clickhouse}
      API_CLICKHOUSE_PORT: 8123
      API_CLICKHOUSE_USER: ${API_CLICKHOUSE_USER:-api}
      API_CLICKHOUSE_PASSWORD: ${API_CLICKHOUSE_PASSWORD}
      API_CLICKHOUSE_DB: ${API_CLICKHOUSE_DB:-analytics}
      API_RATE_LIMIT_PER_MINUTE: ${API_RATE_LIMIT_PER_MINUTE:-100}
      API_CACHE_TTL_SECONDS: ${API_CACHE_TTL_SECONDS:-3600}
    networks:
      - backend_network
      - frontend_network
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=beefy-databarn_frontend_network"
      # Only allow /api/v1 paths through Traefik - blocks /health, /metrics, etc. from external access
      # No stripprefix needed since API routes already include /api/v1 prefix
      - "traefik.http.routers.api.rule=Host(`${API_WEBAPP_DOMAIN_NAME}`) && (PathPrefix(`/api/v1`) || Path(`/docs`) || Path(`/redoc`) || Path(`/openapi.json`))"
      - "traefik.http.routers.api.entrypoints=websecure"
      - "traefik.http.routers.api.tls.certresolver=letsencrypt"
      - "traefik.http.services.api.loadbalancer.server.port=8080"
      - "traefik.http.services.api.loadbalancer.server.scheme=http"
      - "traefik.http.services.api.loadbalancer.healthcheck.path=/health"
      - "traefik.http.services.api.loadbalancer.healthcheck.interval=10s"
      - "traefik.http.services.api.loadbalancer.healthcheck.timeout=5s"
      - "traefik.http.routers.api.service=api"
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
    logging:
      driver: "local"
      options:
        max-size: "1G"
        max-file: "3"


volumes:
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/clickhouse
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/prometheus
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/grafana
  minio_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/minio
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/postgres
  superset_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/superset
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/redis
  traefik_certs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/traefik_certs
  dbt_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/dbt
  dlt_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${STORAGE_DIR}/dlt

networks:
  frontend_network:
    driver: bridge
    name: beefy-databarn_frontend_network
    ipam:
      config:
        - subnet: 172.19.0.0/24
          gateway: 172.19.0.1
  backend_network:
    driver: bridge
    name: beefy-databarn_backend_network
    ipam:
      config:
        - subnet: 172.19.1.0/24
          gateway: 172.19.1.1
